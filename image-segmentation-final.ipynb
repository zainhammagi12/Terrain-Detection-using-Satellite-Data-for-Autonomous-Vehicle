{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing Required Libraries and Modules\nimport os  # For interacting with the operating system\nimport tifffile  # For working with TIFF image files\nimport numpy as np  # For numerical computations and array operations\nimport pandas as pd  # For data manipulation and analysis\nimport tensorflow as tf  # For building and training neural networks\nimport warnings  # For filtering out warnings\nwarnings.filterwarnings('ignore')\nimport cv2  # For computer vision tasks and image processing\nfrom keras.models import Model, load_model  # For creating and loading neural network models\nimport pickle  # For serializing and deserializing Python objects\nimport tensorflow.keras.backend as K  # For low-level TensorFlow operations\nfrom matplotlib import pyplot as plt  # For creating plots and visualizations\nfrom tqdm import tqdm_notebook  # For displaying progress bars in loops\nimport random  # For generating random numbers and operations\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images  # For image reading and manipulation\nimport h5py  # For working with HDF5 files (Hierarchical Data Format)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining Dataset Paths\ntrain_path = \"/kaggle/input/massachusetts-roads-dataset/tiff/train\"\ntrain_mask_path = \"/kaggle/input/massachusetts-roads-dataset/tiff/train_labels\"\nvalid_path = \"/kaggle/input/massachusetts-roads-dataset/tiff/val\"\nvalid_mask_path = \"/kaggle/input/massachusetts-roads-dataset/tiff/val_labels\"\ntest_path = \"/kaggle/input/massachusetts-roads-dataset/tiff/test\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading metadata.csv file\nmeta = pd.read_csv(\"/kaggle/input/massachusetts-roads-dataset/metadata.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading label_class_dict.csv file\nlabel = pd.read_csv('/kaggle/input/massachusetts-roads-dataset/label_class_dict.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting Root Directory\nroot_dir='/kaggle/input/massachusetts-roads-dataset'\n\n#Displaying Random Images and Masks\nfor i in range(10):\n  rand = random.randint(0,len(meta))\n  img_path = os.path.join(root_dir, meta['tiff_image_path'][rand])\n  mask_path = os.path.join(root_dir, meta['tif_label_path'][rand])  \n  image = cv2.imread(img_path)\n  mask = cv2.imread(mask_path)\n  plt.figure(figsize=(10,10))\n  plt.subplot(131)\n  plt.title(\"image\")\n  plt.imshow(image)\n  plt.subplot(132)\n  plt.title(\"mask\")\n  plt.imshow(mask)\n  plt.savefig(\"figure.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing for Images and Masks","metadata":{}},{"cell_type":"code","source":"#Function for saving images with a missing size of less than 10%.\ndef preprocess(meta, thereshold):\n  complete_image_path=[]\n  mask_path=[]\n  for i in range(len(meta)):\n    img_path = os.path.join(root_dir, meta['tiff_image_path'][i])\n    msk_path = os.path.join(root_dir, meta['tif_label_path'][i])\n    image = cv2.imread(img_path)\n    #converting the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    #counting number of white pixels to size of blank part of the image\n    if (np.sum(gray_image==255)/(2250000)) <thereshold:\n      complete_image_path.append(img_path)\n      mask_path.append(msk_path)\n  #print(mask_path)\n  return(complete_image_path, mask_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path, mask_path = preprocess(meta, 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of images remaining: ', len(image_path))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting random images after filtering the dataset\nfor i in range(4):\n  rand = random.randint(0,len(image_path)) \n  image = cv2.imread(image_path[rand])\n  mask = cv2.imread(mask_path[rand])\n  plt.figure(figsize=(10,15))\n  plt.subplot(131)\n  plt.title(\"image\")\n  plt.imshow(image)\n  plt.subplot(132)\n  plt.title(\"mask\")\n  plt.imshow(mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cropping images of 1500x1500 t0 512X512\ndef crop_images(image_path, mask_path, directory, crop):\n  #Creating new directory\n  if os.path.exists(directory):\n    print('Directory alredy exists')\n  else:\n    os.mkdir(directory)\n    print(\"Directory '% s' created\" % directory)\n\n  #Creating new directory to store cropped images\n  if os.path.exists(directory+'/images'):\n    print('Directory alredy exists')\n  else:\n    os.mkdir(directory+'/images')\n    print(\"Directory '% s'/images created\" % directory)\n\n  #Creating new directory to store cropped masks\n  if os.path.exists(directory+'/lables'):\n    print('Directory already exists')\n  else:\n    os.mkdir(directory+'/lables')\n    print(\"Directory '% s'/lables created\" % directory)\n\n  cropped_image_paths = []\n  cropped_mask_paths = []\n\n  for i in tqdm(range(len(image_path))):\n    image = cv2.imread(image_path[i])\n    mask = cv2.imread(mask_path[i])\n    a=0\n    for j in [0,2,4]:\n      for k in [0,2,4]:\n        cropped_image_path = directory+'/images/'+str(a)+'_' + image_path[i].split('/')[-1]\n        croppped_mask_path = directory+'/lables/'+str(a)+'_' + image_path[i].split('/')[-1]\n        a+=1\n\n        cv2.imwrite(cropped_image_path, image[crop[j]:crop[j+1], crop[k]:crop[k+1]])\n        cv2.imwrite(croppped_mask_path, mask[crop[j]:crop[j+1], crop[k]:crop[k+1]])\n\n        cropped_image_paths.append(cropped_image_path)\n        cropped_mask_paths.append(croppped_mask_path)\n\n  return(cropped_image_paths, cropped_mask_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cropping image size\ncrop = [0,512,500,1012,988,1500]\n\n#creating new folder to save cropped images\ndirectory = '/kaggle/working/cropped'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ncropped_images, cropped_masks = crop_images(image_path, mask_path, directory, crop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking length of cropped images\nlen(cropped_masks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting original image with its cropped images\n\n#Original image 1500*1500\nimage = cv2.imread(image_path[0])\nmask = cv2.imread(mask_path[0])\nplt.figure(figsize=(7,10))\nplt.subplot(121)\nplt.title('Original satellite image 1500*1500')\nplt.imshow(image)\nplt.subplot(122)\nplt.title('Original mask 1500*1500')\nplt.imshow(mask)\n\n# cropped image 512*512\nfor i in range(9): \n  image = cv2.imread(cropped_images[i])\n  mask = cv2.imread(cropped_masks[i])\n  plt.figure(figsize=(5,7))\n  plt.subplot(121)\n  plt.title('cropped image 512*512')\n  plt.imshow(image)\n  plt.subplot(122)\n  plt.title('cropped mask 512*512')\n  plt.imshow(mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image Dimensions and Shape Constants.\n\nIMG_HEIGHT = 512\nIMG_WIDTH  = 512\nIMG_CHANNELS = 3\n\ninput_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\nimage_shape = (IMG_HEIGHT, IMG_WIDTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image Preprocessing Function \ndef xyz(cropped_images):\n    x_batch = []\n\n    for i in cropped_images: \n        img = cv2.imread(i)\n        x_batch += [img]\n        \n    x_batch = np.array(x_batch) /255.\n\n    return x_batch\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_mask_image2(image, class_num, color_limit):\n  pic = np.array(image)\n  img = np.zeros((pic.shape[0], pic.shape[1], 1))\n  np.place(img[ :, :, 0], pic[ :, :, 0] >= color_limit, 1)\n  return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mask Preprocessing Function \ndef xyz_01(cropped_masks):\n\n    y_batch = []\n    \n    for i in cropped_masks: \n        mask = cv2.imread(i)\n        mask = preprocess_mask_image2(mask, 2, 50)\n        y_batch += [mask]\n\n    \n  \n    y_batch = np.array(y_batch)\n    \n    return y_batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images=xyz(cropped_images)\nimages.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks=xyz_01(cropped_masks)\nmasks.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" plt.figure(figsize=(20,16))\nx, y = 5,4\nfor i in range(y):  \n    for j in range(x):\n        plt.subplot(y*2, x, i*2*x+j+1)\n        pos = i*120 + j*10\n        plt.imshow(images[pos])\n        plt.title('Sat img #{}'.format(pos))\n        plt.axis('off')\n        plt.subplot(y*2, x, (i*2+1)*x+j+1)\n           \n        #We display the associated mask we just generated above with the training image\n        plt.imshow(masks[pos])\n        plt.title('Mask #{}'.format(pos))\n        plt.axis('off')\n        \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\n#calculates the Intersection over Union (IoU) coefficient between two sets of binary masks\ndef iou_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n    return iou\n\n#calculates the Dice coefficient between two sets of binary masks\ndef dice_coef(y_true, y_pred, smooth = 1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n#Computes the soft Dice loss by subtracting the Dice coefficient from 1.\ndef soft_dice_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(masks.shape)\nprint(images.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting data\n","metadata":{}},{"cell_type":"code","source":"\n#Splitting dataset in 80/20 ratio, where Train=80%, Test=20%\nfrom sklearn.model_selection import train_test_split\ntrain_images, test_images, train_masks, test_masks = train_test_split(images, masks, test_size=0.2, random_state=56)\nprint(\"TRAIN SET\")\nprint(train_images.shape)\nprint(train_masks.shape)\nprint(\"TEST SET\")\nprint(test_images.shape)\nprint(test_masks.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the sizes of the resulting sets\nprint(\"Training set size:\", len(train_images))\n#print(\"Validation set size:\", len(valid_images))\nprint(\"Testing set size:\", len(test_images))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define The Model","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nfrom keras.models import Model, load_model\nimport tensorflow as tf\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom tensorflow.keras.layers import concatenate\nfrom keras import optimizers\nfrom keras.layers import BatchNormalization\nfrom tensorflow.keras.metrics import MeanIoU\nimport keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the input shape for the model\nIMAGE_HEIGHT = IMAGE_WIDTH = 512\nNUM_CHANNELS = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the inputs to the model\ninputs = Input((IMAGE_HEIGHT, IMAGE_WIDTH, 3))\ns = Lambda(lambda x: x / 255) (inputs)\n\n\n# Define the U-Net architecture\n# Encoder\nconv1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\nconv1 = BatchNormalization() (conv1)\nconv1 = Dropout(0.1) (conv1)\nconv1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1)\nconv1 = BatchNormalization() (conv1)\npooling1 = MaxPooling2D((2, 2)) (conv1)\n\nconv2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pooling1)\nconv2 = BatchNormalization() (conv2)\nconv2 = Dropout(0.1) (conv2)\nconv2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2)\nconv2 = BatchNormalization() (conv2)\npooling2 = MaxPooling2D((2, 2)) (conv2)\n\nconv3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pooling2)\nconv3 = BatchNormalization() (conv3)\nconv3 = Dropout(0.2) (conv3)\nconv3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv3)\nconv3 = BatchNormalization() (conv3)\npooling3 = MaxPooling2D((2, 2)) (conv3)\n\nconv4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pooling3)\nconv4 = BatchNormalization() (conv4)\nconv4 = Dropout(0.2) (conv4)\nconv4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv4)\nconv4 = BatchNormalization() (conv4)\npooling4 = MaxPooling2D(pool_size=(2, 2)) (conv4)\n\nconv5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pooling4)\nconv5 = BatchNormalization() (conv5)\nconv5 = Dropout(0.3) (conv5)\nconv5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv5)\nconv5 = BatchNormalization() (conv5)\n\n# Decoder\n# Upsampling and concatenation\nupsample6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (conv5)\nupsample6 = concatenate([upsample6, conv4])\n\n# Convolutional layers in the decoder\nconv6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upsample6)\nconv6 = BatchNormalization() (conv6)\nconv6 = Dropout(0.2) (conv6)\nconv6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv6)\nconv6 = BatchNormalization() (conv6)\n\nupsample7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (conv6)\nupsample7 = concatenate([upsample7, conv3])\nconv7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upsample7)\nconv7 = BatchNormalization() (conv7)\nconv7 = Dropout(0.2) (conv7)\nconv7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv7)\nconv7 = BatchNormalization() (conv7)\n\nupsample8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (conv7)\nupsample8 = concatenate([upsample8, conv2])\nconv8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upsample8)\nconv8 = BatchNormalization() (conv8)\nconv8 = Dropout(0.1) (conv8)\nconv8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv8)\nconv8 = BatchNormalization() (conv8)\n\nupsample9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (conv8)\nupsample9 = concatenate([upsample9, conv1], axis=3)\nconv9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upsample9)\nconv9 = BatchNormalization() (conv9)\nconv9 = Dropout(0.1) (conv9)\nconv9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv9)\nconv9 = BatchNormalization() (conv9)\n\n# Final convolutional layer\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (conv9)\n\n# Create the model\nmodel = Model(inputs=[inputs], outputs=[outputs])\n\n# Print the summary of the model\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{}},{"cell_type":"code","source":"EPOCHS = 100\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callbacks","metadata":{}},{"cell_type":"code","source":"#installing required libraries\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom datetime import datetime","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model Callbacks and Checkpoints Configuration\nmodel_path = \"./Models/road_segmentation_2.h5\"\n\ncheckpointer = ModelCheckpoint(model_path, monitor=\"val_loss\", mode=\"min\", save_best_only = True, verbose=1)\n\nearlystopper = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 1, restore_best_weights = True)\n\nlr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, epsilon=1e-4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compiling the Model","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=\"adam\", loss=soft_dice_loss, metrics=[iou_coef])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training the Segmentation Model\nhistory = model.fit(train_images, train_masks, validation_split = 0.1, epochs=EPOCHS, batch_size = BATCH_SIZE, callbacks = [checkpointer, earlystopper, lr_reducer])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving model\nmodel.save(\"./Models/road_segmentation_final.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing our Model","metadata":{}},{"cell_type":"code","source":"model = load_model(\"./Models/road_segmentation_final.h5\", custom_objects={'soft_dice_loss': soft_dice_loss, 'iou_coef': iou_coef})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluating on test dataset\nmodel.evaluate(test_images, test_masks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prediction\npredictions = model.predict(test_images, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying Prediction Threshold\nthresh_val = 0.1\npredicton_threshold = (predictions > thresh_val).astype(np.uint8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Comparison of Ground Truth and Predictions for Random Samples\nix = random.randint(0, len(predictions))\nnum_samples = 10\n\nf = plt.figure(figsize = (15, 25))\nfor i in range(1, num_samples*4, 4):\n  ix = random.randint(0, len(predictions))\n\n  f.add_subplot(num_samples, 4, i)\n  imshow(test_images[ix][:,:,0])\n  plt.title(\"Image\")\n  plt.axis('off')\n\n  f.add_subplot(num_samples, 4, i+1)\n  imshow(np.squeeze(test_masks[ix][:,:,0]))\n  plt.title(\"Groud Truth\")\n  plt.axis('off')\n\n  f.add_subplot(num_samples, 4, i+2)\n  imshow(np.squeeze(predictions[ix][:,:,0]))\n  plt.title(\"Prediction\")\n  plt.axis('off')\n\n  f.add_subplot(num_samples, 4, i+3)\n  imshow(np.squeeze(predicton_threshold[ix][:,:,0]))\n  plt.title(\"thresholded at {}\".format(thresh_val))\n  plt.axis('off')\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example IoU coefficients for each epoch\niou_coefficients = history.history['iou_coef']\nval_iou_coefficients = history.history['val_iou_coef']\nepochs = range(1, len(iou_coefficients) + 1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(epochs, iou_coefficients,val_iou_coefficients)\nplt.xlabel('Epoch')\nplt.ylabel('IoU Coefficient')\nplt.title('IoU Coefficient vs. Epoch')\n#plt.grid(True)\nplt.xticks(epochs)\nplt.ylim(0, 1)  # Adjust the y-axis range if needed\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Example Loss for each epoch \nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(epochs, loss,val_loss)\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.title('loss vs. Epoch')\n#plt.grid(True)\nplt.xticks(epochs)\nplt.ylim(0, 1) \nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}